# Experiments

Run lazimpa (Parameters provided in the github)

python3 -m egg.zoo.channel.train --dir_save=dir_save \
                                  --impatient=True \
                                  --reg=True \
                                  --vocab_size=40 \
                                  --max_len=30 \
                                  --n_features=100 \
                                  --print_message=False \
                                  --random_seed=7 \
                                  --probs="powerlaw" \
                                  --n_epoch=501 \
                                  --batch_size=512 \
                                  --length_cost=0. \
                                  --sender_cell="lstm" \
                                  --receiver_cell="lstm" \
                                  --sender_hidden=250 \
                                  --receiver_hidden=600 \
                                  --receiver_embedding=100 \
                                  --sender_embedding=10 \
                                  --batches_per_epoch=100 \
                                  --lr=0.001 \
                                  --sender_entropy_coeff=2. \
                                  --sender_num_layers=1 \
                                  --receiver_num_layers=1 \
                                  --early_stopping_thr=0.99

problèmes : dépasse le temps max sur colab, prend 20h sur mon ordi, prend 3h30
sur vast.ai, une plateforme que j'ai testée (mais ça m'a coûté 30 centimes).

Lazimpa : alpha varie de 0 à 0.06
Anti-efficient : alpha va jusqu'à 1 (au dessus pas de convergence)

tranformers : 2 problèmes dans le code à corriger : mettre un nombre de heads qui divise l'embedding (pourquoi ?) supprimer l'argument static_kv dans le forward de egg/core/transformer.py, il ne fait pas partie des arguments (dans l'implémentation de pytorch en tout cas, peut-être dans une ancienne version ?)
prend ~12h sur mon ordi, à tester sur des meilleurs gpu
